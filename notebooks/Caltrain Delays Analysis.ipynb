{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14c053e",
   "metadata": {},
   "source": [
    "# Analyzing Delays on Caltrain\n",
    "by Özge Terzioğlu\n",
    "14 February 2023\n",
    "\n",
    "This is a continuation of my \"Exploring Delays on Caltrain\" notebook, where I documented my process of collecting, filtering, and merging the tweets from @Caltrain and @CaltrainAlerts about train delay on Twitter. \n",
    "\n",
    "In this notebook, we'll be analyzing Caltrain delays via the tweets and documenting preliminary data visualizations about this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b824ef6",
   "metadata": {},
   "source": [
    "## Let's import our tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10d2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import re\n",
    "import altair as alt\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a58f8d",
   "metadata": {},
   "source": [
    "### Read the local file into this notebook to be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c58dc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ozgeterzioglu/tracking-delays-with-caltrain-tweets/data/processed/complete_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cwd \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ozgeterzioglu/tracking-delays-with-caltrain-tweets/data/processed/complete_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m passenger_file \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ozgeterzioglu/tracking-delays-with-caltrain-tweets/data/analysis/passenger_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m passenger_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(passenger_file)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ozgeterzioglu/tracking-delays-with-caltrain-tweets/data/processed/complete_data.csv'"
     ]
    }
   ],
   "source": [
    "cwd = (r\"/Users/ozgeterzioglu/tracking-delays-with-caltrain-tweets/data/raw/complete_data.csv\")\n",
    "passenger_file = (r\"/Users/ozgeterzioglu/tracking-delays-with-caltrain-tweets/data/analysis/passenger_data.csv\")\n",
    "tweets = pd.read_csv(cwd)\n",
    "passenger_data = pd.read_csv(passenger_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c64dfc",
   "metadata": {},
   "source": [
    "## Next, we'll split the date column into day, month, year, and hour of the day for analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54eb5e8",
   "metadata": {},
   "source": [
    "### Change the date and time columns to a datetime object so we can split it into day, month, year, and hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"created_at\"] = pd.to_datetime(tweets[\"created_at\"], format = \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"time\"] = pd.to_datetime(tweets[\"time\"], format = \"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0e6a0",
   "metadata": {},
   "source": [
    "### Insert new columns for day of week, month, year, and hour. Assign the corresponding data into each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ee411",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.insert(11, \"day_of_week\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfbf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"day_of_week\"] = tweets[\"created_at\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.insert(7, \"day\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a486883",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"day\"] = tweets[\"created_at\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.insert(8, \"month\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"month\"] = tweets[\"created_at\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b336ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.insert(9, \"year\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7832d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"year\"] = tweets[\"created_at\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d767a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.insert(10, \"hour\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7197b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"hour\"] = tweets[\"time\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648af9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = tweets[\"created_at\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f882c",
   "metadata": {},
   "source": [
    "## What does our dataframe look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2028513",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639ad10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc58a4c",
   "metadata": {},
   "source": [
    "### Let's change the year data type to string to make analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.astype({\"year\": \"string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e1570",
   "metadata": {},
   "source": [
    "### Now we'll filter our data frame to exclude 2023 (since we only have about a month's worth of data from this year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2012_2022 = tweets[(tweets.year >= \"2012\") & (tweets.year <= \"2022\")]\n",
    "tweets_2012_2022.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad127d",
   "metadata": {},
   "source": [
    "### Let's clean out some extraneous/wrong station names to make our analysis accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2012_2022 = tweets_2012_2022.astype({\"station\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "include = r\"\\bSFK\\b|\\bTWE\\b|\\bBAY\\b|\\bSSF\\b|\\bSBR\\b|\\bMIL\\b|\\bBWY\\b|\\bBUR\\b|\\bSMT\\b|\\bHPK\\b|\\bHIL\\b|\\bBEL\\b|\\bSCS\\b|\\bRWC\\b|\\bMPK\\b|\\bPAL\\b|\\bSTF\\b|\\bCAL\\b|\\bSAT\\b|\\bMVW\\b|\\bSUN\\b|\\bLAW\\b|\\bSCL\\b|\\bCPK\\b|\\bSJD\\b|\\bTAM\\b|\\bCAP\\b|\\bBHL\\b|\\bMHL\\b|\\bSMR\\b|\\bGIL\\b\"\n",
    "exclude = r\"@\"\n",
    "include_mask = tweets_2012_2022[\"station\"].str.contains(include)\n",
    "exclude_mask = tweets_2012_2022[\"station\"].str.startswith(exclude)\n",
    "tweets_2012_2022 = tweets_2012_2022[include_mask & ~exclude_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2012_2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd2f1e",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503711e",
   "metadata": {},
   "source": [
    "### Are there any empty rows in the minutes delayed column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e94dba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tweets[pd.isnull(tweets[\"clean_delay\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89c5bd",
   "metadata": {},
   "source": [
    "Hmmm... it looks like there's about 300 empty rows in the clean_delay row, which is the length of the reported train delay in minutes. It was inevitable to run into some issues filtering tweets and pulling the text from them with regular expressions. We know that I wasn't able to extract delays longer than 2 digits (e.g. 100 minutes delayed). These were pretty rare in the dataset, but still important. I reckon some of these empty rows were triple digit delays. Due to lack of time to fix this further, we will ignore these empty rows for now **(but for an accurate report, the filter.py script should be debugged to catch three digit delay lengths).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b9512",
   "metadata": {},
   "source": [
    "## Let's get started with some basic analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464cebc",
   "metadata": {},
   "source": [
    "### We'll make a few .csv files so I can import them to Tableau or Flourish to create more polished visuals for publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226a92e",
   "metadata": {},
   "source": [
    "We want to see on a large scale, the average number of delays per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_per_year = tweets_2012_2022.groupby(\"year\")[\"clean_delay\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16b39a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alt.Chart(delays_per_year).mark_bar().encode(\n",
    "    x = \"year\",\n",
    "    y = \"clean_delay\",\n",
    "    color = \"clean_delay\"\n",
    ").properties(\n",
    "    title = \"Number of Caltrain Delays Per Year\",\n",
    "    width = 600,\n",
    "    height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff61aba",
   "metadata": {},
   "source": [
    "Seems like the number of delays peaked in 2019, right before the pandemic started. Delays were manually tweeted from 2012-2022. We should ask why this is. We can assume that delays dropped off in 2020 because service was reduced or cancelled due to the pandemic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16429f",
   "metadata": {},
   "source": [
    "per route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19185e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_by_route = tweets_2012_2022.groupby(\"clean_route\")[\"clean_delay\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_by_route.to_csv(\"avg_delays_by_route.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(delays_by_route).mark_bar().encode(\n",
    "    x = \"clean_route\",\n",
    "    y = \"clean_delay\",\n",
    "    color = \"clean_delay\"\n",
    ").properties(\n",
    "    title = \"Average Caltrain Delays Per Route\",\n",
    "    width = 800,\n",
    "    height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71690177",
   "metadata": {},
   "source": [
    "and its median counterpart in case average is wonky due to outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_by_route_median = tweets_2012_2022.groupby(\"clean_route\")[\"clean_delay\"].median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff207b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_by_route_median.to_csv(\"delays_by_route.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(delays_by_route_median).mark_bar().encode(\n",
    "    x = \"clean_route\",\n",
    "    y = \"clean_delay\",\n",
    "    color = \"clean_delay\"\n",
    ").properties(\n",
    "    title = \"Median Number of Caltrain Delays Per Year\",\n",
    "    width = 800,\n",
    "    height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a4bbd3",
   "metadata": {},
   "source": [
    "and lastly, the average number of delays by each unique day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_by_date = tweets_2012_2022.groupby(\"created_at\")[\"clean_delay\"].mean().reset_index().set_axis([\"date\", \"minutes_delayed\"], axis = 1)\n",
    "delays_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_by_date.to_csv(\"delays_by_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(delays_by_date).mark_bar().encode(\n",
    "    x = \"date\",\n",
    "    y = \"minutes_delayed\",\n",
    "    color = \"minutes_delayed\"\n",
    ").properties(\n",
    "    title = \"Average Number of Caltrain Delays Per Year\",\n",
    "    width = 800,\n",
    "    height = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64bc54b",
   "metadata": {},
   "source": [
    "It's going to be hard quantifying the different number of reasons per year since the way the reasons were written is not standardized, but it might be useful to have in case we find the time to standardize the reasons and produce and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab050136",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_by_year = tweets_2012_2022.groupby(\"year\")[\"delay_reason\"].value_counts()\n",
    "reason_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_by_year.to_csv(\"reason_by_year.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cdbdf",
   "metadata": {},
   "source": [
    "total number of delays per station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb66a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay_per_station = tweets_2012_2022.groupby(\"station\")[\"clean_delay\"].count().reset_index().set_axis([\"station\", \"delays_count\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_delay_per_station.to_csv(\"num_delays_by_station.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec989",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alt.Chart(count_delay_per_station).mark_bar().encode(\n",
    "    x = \"station\",\n",
    "    y = \"delays_count\",\n",
    "    color = \"delays_count\"\n",
    ").properties(\n",
    "    title = \"Number of Train Delays Per Station\",\n",
    "    width = 500,\n",
    "    height = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510853fb",
   "metadata": {},
   "source": [
    "Looks like there's very little to no data for some stations. This is a key point we should ask the Caltrains spokesperson about. I know, for instance, that stations like GIL (Gilroy) there is limited service there to begin with, so delays would naturally be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467a3d1",
   "metadata": {},
   "source": [
    "## Let's organize the number of delays by the time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_station = tweets_2012_2022.groupby([\"station\", \"created_at\",\"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73797096",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_hour = tweets_2012_2022.groupby([\"created_at\",\"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stationdelays_bytime.csv\", mode = \"w\") as file:\n",
    "    writer = csv.writer(file, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    writer.writerow([\"station\",\"date\",\"hour\",\"avg_delay\"])\n",
    "    for station_key, station_item in grouped_station:\n",
    "        grouped_delay = grouped_station.get_group(station_key)[\"clean_delay\"].median()\n",
    "        station_name = station_key[0]\n",
    "        date = station_key[1].strftime(\"%Y-%m-%d\")\n",
    "        hour = station_key[2]\n",
    "        writer.writerow([station_name, date, hour, grouped_delay])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86295b0e",
   "metadata": {},
   "source": [
    "## Since 2012, how many tweets provided context or reasoning for the delay? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_count = tweets_2012_2022[\"delay_reason\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2012_2022.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 10420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_count / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bfe9f",
   "metadata": {},
   "source": [
    "This is something to ask Caltrain about, why are commuters notified of the cause of the train delay only 5% of the time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a2117",
   "metadata": {},
   "source": [
    "## How many passengers were affected by delays on Caltrain from 2016-2019?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348b63a",
   "metadata": {},
   "source": [
    "Let's see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659cbb5",
   "metadata": {},
   "source": [
    "Change the column names to snake case for smooth analysis and change the data type for `year` from integer to string so it's displayed properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_data.columns = [col.lower().replace(\" \",\"_\") for col in passenger_data.columns]\n",
    "passenger_data = passenger_data.astype({\"year\": \"string\"})\n",
    "passenger_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3fd21",
   "metadata": {},
   "source": [
    "Looks like we're working with a dataset that spans from 2012-2019. Since 2012-2015 doesn't have data for the \"On Board\" column, which is the column we want to look at when quantifying how many passengers were affected by delays, we will filter out these years and do a short-term analysis instead. I know this data is missing because I created this dataset myself since I had to pull from multiple pdf files and spreadsheets to create it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2f0fa",
   "metadata": {},
   "source": [
    "We'll also have to edit the station names to match the tweets dataframe. We'll do this because we're going to merge these two dataframes together soon and create some viz with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_passengers = passenger_data[(passenger_data.year >= \"2016\") & (passenger_data.year <= \"2019\")]\n",
    "filtered_passengers = filtered_passengers.replace({\n",
    "    'San Francisco' : 'SFK',\n",
    "    '22nd Street' : 'TWE',\n",
    "    \"Bayshore\" : \"BAY\",\n",
    "    \"South San Francisco\" : \"SSF\",\n",
    "    \"San Bruno\" : \"SBR\",\n",
    "    \"Millbrae\" : \"MIL\",\n",
    "    \"Broadway\" : \"BWY\",\n",
    "    \"Burlingame\" : \"BUR\",\n",
    "    \"San Mateo\" : \"SMT\",\n",
    "    \"Hayward Park\" : \"HPK\",\n",
    "    \"Hillsdale\" : \"HIL\",\n",
    "    \"Belmont\" : \"BEL\",\n",
    "    \"San Carlos\" : \"SCS\",\n",
    "    \"Redwood City\" : \"RWC\",\n",
    "    \"Atherton\" : \"ATH\",\n",
    "    \"Menlo Park\" : \"MPK\",\n",
    "    \"Palo Alto\" : \"PAL\",\n",
    "    \"California Avenue\" : \"CAL\",\n",
    "    \"San Antonio\" : \"SAT\",\n",
    "    \"Mountain View\" : \"MVW\",\n",
    "    \"Sunnyvale\" : \"SUN\",\n",
    "    \"Lawrence\" : \"LAW\",\n",
    "    \"Santa Clara\" : \"SCL\",\n",
    "    \"College Park\" : \"CPK\",\n",
    "    \"San Jose Diridon\" : \"SJD\",\n",
    "    \"Tamien\" : \"TAM\",\n",
    "    \"Capitol\" : \"CAP\",\n",
    "    \"Blossom Hill\" : \"BHL\",\n",
    "    \"Morgan Hill\" : \"MHL\",\n",
    "    \"San Martin\" : \"SMR\",\n",
    "    \"Gilroy\" : \"GIL\"\n",
    "    \n",
    "})\n",
    "filtered_passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71feb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(passengers_onboard).mark_line().encode(\n",
    "    x = \"year\",\n",
    "    y = \"on_board\"\n",
    ").properties(\n",
    "    title = \"Average Number of On Board Passengers Riding Caltrain from All Stations\",\n",
    "    width = 300,\n",
    "    height = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f61cb",
   "metadata": {},
   "source": [
    "The number of passengers riding Caltrain seems pretty stable between 2016-2019 and even on a slight incline. It would be more useful if we could see how ridership is affected by the pandemic. \n",
    "\n",
    "Let's compare the ridership from each station from 2016-2019 with delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed50cb7",
   "metadata": {},
   "source": [
    "We'll first filter our delays dataset for 2016-2019 to match the passenger dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967dcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2016_2019 = tweets[(tweets.year >= \"2016\") & (tweets.year <= \"2019\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8ece8",
   "metadata": {},
   "source": [
    "Then we'll clean up our station names again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "include = r\"\\bSFK\\b|\\bTWE\\b|\\bBAY\\b|\\bSSF\\b|\\bSBR\\b|\\bMIL\\b|\\bBWY\\b|\\bBUR\\b|\\bSMT\\b|\\bHPK\\b|\\bHIL\\b|\\bBEL\\b|\\bSCS\\b|\\bRWC\\b|\\bMPK\\b|\\bPAL\\b|\\bSTF\\b|\\bCAL\\b|\\bSAT\\b|\\bMVW\\b|\\bSUN\\b|\\bLAW\\b|\\bSCL\\b|\\bCPK\\b|\\bSJD\\b|\\bTAM\\b|\\bCAP\\b|\\bBHL\\b|\\bMHL\\b|\\bSMR\\b|\\bGIL\\b\"\n",
    "exclude = r\"@\"\n",
    "include_mask = tweets_2016_2019[\"station\"].str.contains(include).fillna(False)\n",
    "exclude_mask = tweets_2016_2019[\"station\"].str.startswith(exclude).fillna(False)\n",
    "tweets_2016_2019 = tweets_2016_2019[include_mask & ~exclude_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7599454",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2016_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27aea4",
   "metadata": {},
   "source": [
    "Now that we're certain the station names are standardized, we'll organize our dataframe on station delays to have the number of delays reported at that station that year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c02e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_per_station = tweets_2016_2019[[\"station\", \"clean_delay\", \"year\"]].copy()\n",
    "station_delays_groupby = delays_per_station.groupby([\"station\", \"year\"])\n",
    "delays_per_station = station_delays_groupby.mean().reset_index()\n",
    "delays_per_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf12de4",
   "metadata": {},
   "source": [
    "Now, we'll merge this dataframe with the one about the average weekly number of passengers on board at each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85129d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_merged = pd.merge(delays_per_station, filtered_passengers, on = [\"station\", \"year\"])\n",
    "passenger_merged\n",
    "passenger_merged.to_csv(\"passenger_and_delays_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60181433",
   "metadata": {},
   "source": [
    "Let's create a viz and see roughly how many passengers were affected by delays at each station from 2016-2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2b1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alt.Chart(passenger_merged).mark_bar().encode(\n",
    "    x = \"clean_delay\",\n",
    "    y = \"station\",\n",
    "    color = \"year\"\n",
    ").properties(title = \"Average Station Delays on Caltrain\", height = 500, width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1563da",
   "metadata": {},
   "source": [
    "It's strange that 8 stations don't have any tweeted train delays for some of the years throughout 2016-2019. This would be something to ask Caltrain about and also double check our dataset for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(passenger_merged).mark_bar().encode(\n",
    "    x = \"year\",\n",
    "    y = \"on_board\",\n",
    ").properties(title = \"Average Number of Caltrain Riders 2016-2019\", height = 400, width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982287da",
   "metadata": {},
   "source": [
    "It appears that the average number of on board passengers for Caltrain remained pretty consistent from 2016-2019. This shows that thousands of riders every week were affected by Caltrain delays, especially at 22nd Street station in San Francisco, where the most train delays were tweeted in this time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7041d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f15ebd3f221a1facee53b815257ffdbfeb0283cf9fe39534b9eb36b2c2b1d2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
