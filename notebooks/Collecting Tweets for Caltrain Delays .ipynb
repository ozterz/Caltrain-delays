{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd03f749",
   "metadata": {},
   "source": [
    "# Exploring Delays on Caltrain\n",
    "by Özge Terzioğlu (ozterz@stanford.edu | www.github.com/ozterz)\n",
    "1 March 2023\n",
    "\n",
    "Caltrain provides commuters from San Francisco to San Jose and Gilroy with commuter rail service. Many Bay Areans rely on Caltrain to get to work, the airport, sporting events, and more. Caltrain states on its website that its vision is to: \"Provide a safe, reliable, sustainable modern rail system that meets the growing mobility needs of the San Francisco Bay Area region.\" \n",
    "\n",
    "However, many students who rely on Caltrain to get to San Francisco and commuters alike often criticize Caltrain for being consistently late, delayed, and therefore unreliable. \n",
    "\n",
    "Caltrain has been tweeting train delays in real time since 2012, first from its @Caltrain account and now from the @CaltrainAlerts account as of October 1, 2020. The tweets regarding delays are now \"hybrid\" as of September 2022, meaning most of the delays are automated but if a human is available they will tweet out reasons for delays, as the context is not an automated feature. \n",
    "\n",
    "Caltrain hopes to improve the reliability of its trains and delay tweets when its electrification project finishes in 2024. Newer trains will provide quicker, more reliable service with updated GPS features allowing tweets to be faster and more accurate for commuters. \n",
    "\n",
    "For this project, I collected all of the tweets from @Caltrain and @CaltrainAlerts using Twitter's API and the python library tweepy (made specifically for calling Twitter APIs).\n",
    "\n",
    "This dataset spans from 2012 to early 2023. \n",
    "\n",
    "I collected the tweets in January in the last days of it being cost-free using an Academic Access Developer account (I had to apply for this but I was approved within 48 hours)... my luck must be good! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb86ce",
   "metadata": {},
   "source": [
    "## Let's pull tweets using Twitter's API (main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac08301",
   "metadata": {},
   "source": [
    "### Importing our tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cedfe7",
   "metadata": {},
   "source": [
    "### Define our authentication token variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f017410",
   "metadata": {},
   "source": [
    "Disclaimer: I learned in class a few weeks later that I could store it locally on my file and not push it to Github, when I did this via self-learning I inserted it directly into my code-- do NOT do this for security reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = \"INSERT_YOUR_TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e13e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token = bearer_token, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d08a01",
   "metadata": {},
   "source": [
    "### Create a csv file where we'll store our collected tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da0c3b",
   "metadata": {},
   "source": [
    "The column headers were named by looking at the API documentation and seeing which data it collects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"main_account_data.csv\", mode = \"w\") as tweet_file:\n",
    "    tweet_writer = csv.writer(tweet_file, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    tweet_writer.writerow([\"created_at\",\"id\",\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687487e9",
   "metadata": {},
   "source": [
    "Now, let's use a for loop to loop through all the tweets from the @Caltrain account since it's creation until the date they stopped tweeting delays from this account. In the nested for loop we are opening the file \"main_account_data.csv\" that we created above and *appending* each tweet as a new row there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b498d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tweepy.Paginator(client.search_all_tweets, query = 'from:Caltrain', max_results = 500, start_time = \"2006-03-21T00:00:00Z\", end_time = \"2020-10-01T00:00:00Z\", tweet_fields = [\"created_at\",\"id\",\"text\"]):\n",
    "    for tweets in item.data:\n",
    "        with open(\"main_account_data.csv\", mode = \"a\") as tweet_file:\n",
    "            tweet_writer = csv.writer(tweet_file, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "            tweet_writer.writerow([tweets[\"created_at\"], tweets[\"id\"], tweets[\"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a149a",
   "metadata": {},
   "source": [
    "To pull the tweets from the @CaltrainAlerts account, I used the same script, but replaced \"query = \"from:Caltrain\"\" with \"from:CaltrainAlerts. I also created and opened a new file to save this account's tweets to called \"alert_acct_tweets.csv\" to keep each account's data separate so I could check for errors before merging the two files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced8d48",
   "metadata": {},
   "source": [
    "## Time to filter tweets for delays (filter.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf66c17",
   "metadata": {},
   "source": [
    "Since the main @Caltrain account from 2012-2022 tweeted about both delays and other Caltrain-related notifications, we need to filter only for delays. To decide what to filter for, I scanned the collected tweets looking for consistent formatting. I filtered for words pertaining to delays, like \"late\", \"waiting\", \"terminated\", \"single tracking\", as well as words pertaining to incidents like \"incident\" or \"emergency\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf677d7b",
   "metadata": {},
   "source": [
    "### Importing our tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e06a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c82aa7",
   "metadata": {},
   "source": [
    "### Identify the path to the csv files holding the tweets on our local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(__file__).parent\n",
    "localfile = Path.joinpath(cwd, \"main_account_data.csv\")\n",
    "outfile = Path.joinpath(cwd, \"filtered_tweets.csv\")\n",
    "alert_acct_data = Path.joinpath(cwd, \"alert_acct_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123425e",
   "metadata": {},
   "source": [
    "Outfile will be used to store the tweets containing notifications about delays only, hence the name \"filtered_tweets.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91cf7d6",
   "metadata": {},
   "source": [
    "### Combine the csv files holding tweets from @Caltrain and @CaltrainAlerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26dfb5",
   "metadata": {},
   "source": [
    "Since both files have the same column names, we don't need to to anything to prepare for the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainacct_tweets = pd.read_csv(localfile)\n",
    "alert_tweets = pd.read_csv(alert_acct_data)\n",
    "mainacct_tweets = pd.concat([mainacct_tweets, alert_tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1629d",
   "metadata": {},
   "source": [
    "### Use regular expressions to declare what you want to filter the tweets for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fe83d",
   "metadata": {},
   "source": [
    "I want the regex to look for aforementioned key words pertaining to delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "include = \"[0-9]+[\\'\\\"]|[-][0-9]+|delay|\\blate\\b|\\bwaiting\\b|\\bterminated\\b|\\bstopped\\b|\\bemergency\\b|\\bstruck\\b|\\bc[as]ncel+ed\\b|\\bbroke down\\b|\\bdisabled\\b|\\bfatality\\b|\\bkilled\\b|\\bslow\\b|\\bdown\\b|\\bsingle track\\b|\\bsingle tracking\\b|\\boff loading\\b|\\bdrop\\b|\\bdead\\b|\\bstop\\b|\\bstalled\\b|\\bmechanical issue\\b|\\bincident\\b|\\bholding\\b|\\brestriction\\b|\\bannulled\\b|\\bnot running\\b|\\bclosed\\b\"\n",
    "exclude = tuple([\"@\", \"RT\"])\n",
    "exclude_regex = r\"\\bBoard of Directors\\b|\\bPublic Session\\b|\\bzoom\\b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dffa1ac",
   "metadata": {},
   "source": [
    "Set variables that filter the \"text\" column using the above regex variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2de815",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_mask = mainacct_tweets[\"text\"].str.contains(include)\n",
    "exclude_mask = mainacct_tweets[\"text\"].str.startswith(exclude)\n",
    "exclude_words = mainacct_tweets[\"text\"].str.contains(exclude_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c32a1",
   "metadata": {},
   "source": [
    "Store the filtered tweets to the outfile we created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec83c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets = mainacct_tweets[(include_mask & ~exclude_mask) & ~exclude_words]\n",
    "filtered_tweets.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eebbc1",
   "metadata": {},
   "source": [
    "## Separating the \"created_at\" date into two columns (time and date) (clean.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b41339",
   "metadata": {},
   "source": [
    "This step is necessary if we want to aggregate delays by the time of day or year or day, etc. Datetime can be annoying so hang in tight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6330fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e9559",
   "metadata": {},
   "source": [
    "As usual, let's read the local file with our data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1746f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(__file__).parent\n",
    "localfile = Path.joinpath(cwd, \"filtered_tweets.csv\")\n",
    "all_tweets = pd.read_csv(localfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee12f34",
   "metadata": {},
   "source": [
    "Let's create a new column where we'll store the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.insert(2, \"time\", None, allow_duplicates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd3eaf",
   "metadata": {},
   "source": [
    "When we merged our two csv files together, the indexes from the previous files combined, so the numbers became meaningless. Let's get rid of that for the sake of my sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_tweets[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b8c90",
   "metadata": {},
   "source": [
    "Time to get to business. First, we'll establish the timezone the tweets were collected in (UTC) and the timezone we want them to be displayed as (PST). I know the tweets were collected in UTC because I read the API documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99998636",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTC = timezone(\"UTC\")\n",
    "PST = timezone(\"America/Los_Angeles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1b7de",
   "metadata": {},
   "source": [
    "Now we'll cycle through every row and do the following:\n",
    "1) read in the created_at string into a datetime object so we can manipulate it.\n",
    "2) Let the datetime object know that the current timezone is UTC.\n",
    "3) Create a clone of the datetime object and convert that to PST.\n",
    "4) Have the cloned object spit out a string with the date in YYYY-MM-DD format. One for the date, one for the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in all_tweets.index:\n",
    "    utc_time = datetime.strptime(all_tweets[\"created_at\"][tweet], \"%Y-%m-%d %H:%M:%S+00:00\") \n",
    "    utc_time = utc_time.replace(tzinfo = UTC) \n",
    "    pst_time = utc_time.astimezone(PST)\n",
    "    date = pst_time.strftime(\"%Y-%m-%d\")\n",
    "    time = pst_time.strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbd598",
   "metadata": {},
   "source": [
    "Next we'll simply update the respective columns with the newly separated date and time and store it in our csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets[\"created_at\"][tweet] = date \n",
    "all_tweets[\"time\"][tweet] = time\n",
    "all_tweets.to_csv(\"filtered_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb7d4b",
   "metadata": {},
   "source": [
    "## You thought we were done filtering the text? Think again. (filter_text.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee40d2",
   "metadata": {},
   "source": [
    "At this point, the important information in each tweet is all packed into a single column: \"text\". It would be difficult to do a deep analysis without having the minutes delayed, the approaching station, route number, and reason for the delay in their own separate columns. We must head into the weeds using regular expressions (regex). Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5628345",
   "metadata": {},
   "source": [
    "### Importing our tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad701aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3f94b",
   "metadata": {},
   "source": [
    "Define path for our data files from our local machine to Python. Establish an outfile as a home where the new dataframe we'll create in this script will live. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(__file__).parent\n",
    "filtered_tweets = Path.joinpath(cwd, \"NEW_filtered_tweets.csv\")\n",
    "outfile = Path.joinpath(cwd, \"analyze_tweets.csv\")\n",
    "start_file = pd.read_csv(filtered_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0167d9d",
   "metadata": {},
   "source": [
    "Define our regex variables aka what do we want to look for and eventually pull from the \"text\" column in our filtered_tweets.csv file? \n",
    "\n",
    "**Huge Disclaimer:** This step took a week or so. I meticulously scoured through the csv file of about 10,000 rows looking for the different variations of how each component was written. Since the delay tweets were manually written (human created) for 10 years, there was little standardization of how delays were tweeted out. Inevitably, this dataset may not be entirely accurate or reflective of the actual delays. For one, I couldn't grab delays in the triple digits (when I looked through the csv file, there were few of them, but they are still crucial to have as data). I modified the regex variables a lot with the help of https://regexr.com/. Filtering text is a huge pain in the butt, if there's any easier way to go about this, I would love to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_num = r\"\\bNo. [0-9]\\b|\\b#[0-9]\\b|[0-9]{3}|#[A-Za-z]{2}[0-9]+\\b|\\b[A-Za-z]{2}[0-9]\\b|\\b[0-9]{3} -\"\n",
    "minutes_delayed = r\"-?[0-9]+[\\'\\\"]|[-][0-9]+|[0-9]{2}.|[0-9]{2,3} min\"\n",
    "delay_reason = r\"due to\\b|\\bbecause of\\b|\\bfollowing\\b|\\b\"\n",
    "station = r\"@?[A-Z][aA-zZ][A-Z]\\b\"\n",
    "terminated = r\"terminated\\b|\\bwill terminate\\b|\\bis terminating\\b|\\bcanceled\\b|\\bcancelled\\b|\\bannulled\\b\"\n",
    "grab_reason = r\"(?<=due to )(\\w| )*(?:.)|(?<=because of )(\\w| )*(?:.)|(?<=following )(\\w| )*(?:.)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60aeeed",
   "metadata": {},
   "source": [
    "Now that's out of the way, let's open the csv file where we'll store these newly filtered key items in their own respective columns. In the for loop, we will:\n",
    "1) Loop through our starting file, \"filtered_tweets.csv\", and use the regular expressions module to set a condition for each item of interest in the \"text\" column (route number, minutes delayed, delay reason, station, and if the train was terminated). Since the most consistent data point was the route number, we will pull each of our other items of interest in relation to the route number identified in the tweet to ensure that the information in each row has a relation. \n",
    "2) If the route number is identified in the \"text\" column via the regex variable we made for filtering (found_route), it will be added to found_route_substr. \n",
    "3) Then, we will look for the other items of interest associated to this route number. If the item of interest is not found in the string, we will set it equal to an empty string. We will do this for minutes, reason, and station.\n",
    "4) We wanted a column indicating if a train was terminated or not. If the word terminated is found in the string, we will assign that found string to the terminated column. If it's not found, we will write \"No\" to the row. \n",
    "5) Lastly, we only want data that has at least a route number AND minutes delayed. So if there's no reason, no terminated status, or no minutes delayed, that piece of associated information will be written to our outfile for analysis (analyze_tweets.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"analyze_tweets.csv\", mode = \"w\") as new_file:    \n",
    "        outfile = csv.writer(new_file, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        outfile.writerow([\"created_at\",\"time\",\"route\",\"station\",\"delay_length\",\"delay_reason\",\"terminated_train\"])\n",
    "\n",
    "        for tweet in start_file.index:\n",
    "                if re.search(route_num, start_file.loc[tweet,\"text\"]):\n",
    "                    route_str = start_file.loc[tweet,\"text\"]\n",
    "                    found_route = re.findall(route_num, route_str)\n",
    "                    found_route_substr = re.split(route_num, route_str)\n",
    "                    found_route_substr = found_route_substr[1:]\n",
    "\n",
    "                    for index in range(0,found_route_substr.__len__()):\n",
    "                        found_mins = None\n",
    "                        found_reason = None\n",
    "                        found_station = None\n",
    "                        found_term = None\n",
    "\n",
    "                        if re.search(minutes_delayed, found_route_substr[index]):\n",
    "                            min_str = found_route_substr[index]\n",
    "                            found_mins = re.findall(minutes_delayed, min_str)[0]\n",
    "                            if re.findall(minutes_delayed, min_str).__len__() > 1:\n",
    "                                    print(min_str)\n",
    "                                    print(re.findall(minutes_delayed, min_str))\n",
    "                        else:\n",
    "                            found_mins = \"\" \n",
    "\n",
    "                        if re.search(delay_reason, found_route_substr[index], flags = re.IGNORECASE):\n",
    "                            reason_str = found_route_substr[index]\n",
    "                            found_iter = re.finditer(grab_reason, reason_str, flags = re.IGNORECASE)\n",
    "                            for search in found_iter:\n",
    "                                found_reason = search.string[search.start():search.end()]\n",
    "                            if found_reason == None:\n",
    "                                found_reason = \"\"\n",
    "                        else: \n",
    "                             found_reason = \"\"\n",
    "                    \n",
    "                        if re.search(station, found_route_substr[index]):\n",
    "                            station_str = found_route_substr[index]\n",
    "                            found_station = re.findall(station, station_str)[0]\n",
    "                        else:\n",
    "                             found_station = \"\"\n",
    "                            \n",
    "                        if re.search(terminated, found_route_substr[index]):\n",
    "                            term_str = found_route_substr[index]\n",
    "                            found_term = re.findall(terminated, term_str)[0]\n",
    "                        else:\n",
    "                             found_term = \"No\"\n",
    "\n",
    "                        if found_mins != \"\" or found_reason != \"\" or found_term != \"No\":\n",
    "                            outfile.writerow([start_file.loc[tweet,\"created_at\"],start_file.loc[tweet,\"time\"],found_route[index],found_station,found_mins,found_reason,found_term])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22fdab",
   "metadata": {},
   "source": [
    "## Are we there yet? No. Since we pulled phrases from long strings, it's bound to be messy. Grab a mop and let's get to work. (format_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ae2d6",
   "metadata": {},
   "source": [
    "### Grabbing our cleaning supplies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540159aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd \n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7e37f",
   "metadata": {},
   "source": [
    "Creating a path from our local machine to Python for our files to travel here on. We've created quite the path of files up to this point. Let's write whatever manipulated data we'll end up creating to the same csv (analyze_tweets.csv) because we're just cleaning up the formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(__file__).parent\n",
    "start_file = Path.joinpath(cwd, \"analyze_tweets.csv\")\n",
    "tweets = pd.read_csv(start_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384969dd",
   "metadata": {},
   "source": [
    "Let's create news columns to store what will be our cleaned up data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"clean_route\" not in tweets.columns:\n",
    "    tweets.insert(3, \"clean_route\", None, allow_duplicates = True)\n",
    "    tweets.insert(5, \"clean_delay\", None, allow_duplicates = True)\n",
    "    tweets.insert(7, \"clean_terminated\", None, allow_duplicates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0620dc0",
   "metadata": {},
   "source": [
    "We're starting with the train route. If the route has an extraneous characters like the hashtag, we want to remove it for ease of analysis. When I scanned the data in the beginning steps, it looked like route numbers were either just numbers or started with a hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df473d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, search in enumerate(tweets[\"route\"]):\n",
    "    flag = r\"#\"\n",
    "    clean_route = re.sub(flag, \"\", search)\n",
    "    tweets.at[index, \"clean_route\"] = clean_route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146f964",
   "metadata": {},
   "source": [
    "Next, we'll move onto the length of delay (which was plotted in minutes). When scanning the dataset, I found that oftentimes there would be random upper or lowercase letters attached to the beginning or end of the minutes, so let's remove them so we can treat the clean_delay column like floating point numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, search in enumerate(tweets[\"delay_length\"]):\n",
    "    filter = r\"\\D|[aA-zZ]\"\n",
    "    if type(search) == float:\n",
    "        search = str(search)\n",
    "    clean_mins = re.sub(filter, \"\", search)\n",
    "    tweets.at[index, \"clean_delay\"] = clean_mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100723a",
   "metadata": {},
   "source": [
    "A terminated train was denoted in many different ways (terminated, annulled, canceled, or cancelled). Let's just replace this with \"yes\" for simplicity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, search in enumerate(tweets[\"terminated_train\"]):\n",
    "    flag_2 = r\"terminated|annulled|canceled|cancelled\"\n",
    "    clean_term = re.sub(flag_2, \"Yes\", search)\n",
    "    tweets.at[index, \"clean_terminated\"] = clean_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55138809",
   "metadata": {},
   "source": [
    "Lastly, we'll send these freshly cleaned up tweets back to our \"analyze_tweets.csv\" for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b0be7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtweets\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(start_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets.to_csv(start_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda5f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 16:23:03) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f15ebd3f221a1facee53b815257ffdbfeb0283cf9fe39534b9eb36b2c2b1d2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
